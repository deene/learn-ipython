{
 "metadata": {
  "name": "",
  "signature": "sha256:d7b06694004e7a2a6745a017f0cc986417956288df0de7a5e819bf021db9c1c1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# This notebook is dedicated to the study of data mining"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How to download from Nature\n",
      "\n",
      "The below code downloads data from nature and save into json."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How to parse XML in Python?\n",
      "\n",
      "[This stackoverflow post](http://stackoverflow.com/questions/1912434/how-do-i-parse-xml-in-python) explains how to use Python to parse XML.\n",
      "\n",
      "minidom is the quickest and pretty straight forward:\n",
      "\n",
      "```XML\n",
      "<data>\n",
      "    <items>\n",
      "        <item name=\"item1\"></item>\n",
      "        <item name=\"item2\"></item>\n",
      "        <item name=\"item3\"></item>\n",
      "        <item name=\"item4\"></item>\n",
      "    </items>\n",
      "</data>\n",
      "```\n",
      "\n",
      "```Python\n",
      "from xml.dom import minidom\n",
      "xmldoc = minidom.parse('items.xml')\n",
      "itemlist = xmldoc.getElementsByTagName('item') \n",
      "print len(itemlist)\n",
      "print itemlist[0].attributes['name'].value\n",
      "for s in itemlist :\n",
      "    print s.attributes['name'].value\n",
      "```\n",
      "\n",
      "```Output\n",
      "4\n",
      "item1\n",
      "item1\n",
      "item2\n",
      "item3\n",
      "item4\n",
      "```\n",
      "\n",
      "And I can use the ```s.childNodes[0].nodeValue``` to get the inner value.\n",
      "\n",
      "Below is the test code working on the nature-2010.xml that I downloaded from Nature. I can use the [intentXML](https://github.com/alek-sys/sublimetext_indentxml) sublime plugin to make the XML file look nicer."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But this code does not work as the xml file downloaded from nature is kind of [junky](http://stackoverflow.com/questions/26008046/xml-parsing-error-junk-after-document-element-in-twilio).\n",
      "\n",
      "So instead, I try the json approach. (use the [prettyJSON](https://github.com/dzhibas/SublimePrettyJson) sublime plugin to make the json file look nicer."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Code to crawl Nature publications.\n",
      "\n",
      "The following code uses the Nature API to search for papers published in Nature."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import xml.etree.ElementTree as ET\n",
      "import urllib2, urllib\n",
      "import codecs\n",
      "import time\n",
      "\n",
      "content = None\n",
      "# build up a product code array, can be found at the A-Z page links on nature.com\n",
      "# build up a year=month=day array\n",
      "\n",
      "# Crawl nature\n",
      "print \"\\n\\nstart -----\"\n",
      "for year in range(2012, 2013):\n",
      "    fileName = 'nnano-' + str(year) + '.json'\n",
      "    myfile = open(fileName, 'w+')\n",
      "    myfile.write('[')\n",
      "    yearWord = str(year)\n",
      "\n",
      "    for month in range(1,13):\n",
      "        if month < 10:\n",
      "            monthWord =  '0' + str(month)\n",
      "        else:\n",
      "            monthWord = str(month)\n",
      "\n",
      "        for day in range(1,32):\n",
      "            if day<10:\n",
      "                dayWord = '0' + str(day)\n",
      "            else:\n",
      "                dayWord = str(day)\n",
      "\n",
      "            targetDate = yearWord + '-' + monthWord + '-' + dayWord\n",
      "            print targetDate,\n",
      "        \n",
      "            queryWord = 'prism.productCode=nnano+AND+prism.publicationDate=' + targetDate\n",
      "            url = 'http://api.nature.com/content/opensearch/request?&recordPacking=unpacked&queryType=cql&maximumRecords=100&httpAccept=application/json&query=' + queryWord\n",
      "\n",
      "            try:\n",
      "                content = urllib2.urlopen(url).read()\n",
      "                data = json.loads(content)\n",
      "            except:\n",
      "                print \"URL error\"\n",
      "                continue\n",
      "                    \n",
      "            if data['feed']['opensearch:totalResults'] != 0:\n",
      "                for entry in data['feed'][\"entry\"]:\n",
      "                    #print entry[\"prism:publicationName\"] + ',' + entry[\"prism:publicationDate\"] + ',' + entry[\"dc:title\"] \\\n",
      "                    #      + ':' + entry[\"dc:description\"]\n",
      "                    json.dump(entry, myfile, indent=4)\n",
      "                    myfile.write(',')\n",
      "                #myfile.write(simplejson.dumps(simplejson.loads(output), indent=4, sort_keys=True))\n",
      "            else:\n",
      "                print \"No result found!\"\n",
      "            \n",
      "            time.sleep(1)\n",
      "    \n",
      "    myfile.write(']')\n",
      "    myfile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "start -----\n",
        "2012-01-01 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "URL error\n",
        "2012-01-02 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No result found!\n",
        "2012-01-03"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " No result found!\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-d0e2d139ac6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"No result found!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mmyfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m']'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And the below code will crawl IEEE and Springer accordingly"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## crawl IEEE Xplore\n",
      "#keyword = urllib.quote_plus(\"graphene\")\n",
      "#url = \"http://ieeexplore.ieee.org/gateway/ipsSearch.jsp?&hc=1&querytext=\" + keyword\n",
      "#content = urllib2.urlopen(url).read()\n",
      "#tree = ET.fromstring(content)\n",
      "#for child in tree.iter('title'):\n",
      "#   print child.text\n",
      "#\n",
      "## crawl Springer\n",
      "#url = 'http://api.springer.com/metadata/json?p=100&q=title:graphene&api_key=3a3sxqg25fmwnzrsy2qmgjds'\n",
      "#content = urllib2.urlopen(url).read()\n",
      "#data = json.loads(content)\n",
      "#for record in data['records']:\n",
      "#   print record['title'].encode('utf-8')\n",
      "#   #print record['doi'].encode('utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Codes to analyze the downloaded json file\n",
      "\n",
      "Now I need to take in the description of each entry from the json file and feed it into the WordCount class. In order to count the occurance of words in all texts, I need to create a WordCount class. This class should have a function which takes in a certain string, count the occurance of each word in that string, and add the result to the previously stored values in that class. So below is the code to make such a class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "import json\n",
      "from pprint import pprint\n",
      "\n",
      "class WordCount:\n",
      "    \"\"\"A simple word count class\"\"\"\n",
      "    def __init__(self):\n",
      "        self.word_table = {}\n",
      "        self.msg = ''\n",
      "        self.ignore_list = {}\n",
      "#         self.ignore_list = ('of', 'the', 'a', 'to', 'and', 'in', 'be', 'as', \\\n",
      "#                             'that', 'this', 'can', 'for', 'have', 'is', 'with', \\\n",
      "#                             'by', 'are', 'used', 'such', 'on', 'it', 'at', 'from', \\\n",
      "#                             'high', 'has', 'an', 'which', '(', ')', ',', 'made', 'its', \\\n",
      "#                             'new', '2013', '.', 'two', '-', 'researchers', 'been', 'or', \\\n",
      "#                             'using', 'state', '\\\u2014', 'not')\n",
      "\n",
      "    def append(self, string):\n",
      "        # print 'Analyzing...'\n",
      "        new_word_array = split_string.split()\n",
      "        \n",
      "        for aword in new_word_array:\n",
      "            # ignore the normal words\n",
      "            if aword in self.ignore_list:\n",
      "                continue\n",
      "            if aword in self.word_table:\n",
      "                # if the word exists, add 1\n",
      "                self.word_table[aword] = self.word_table[aword] + 1\n",
      "            else:\n",
      "                # if the word does not exist, init the key with 1\n",
      "                self.word_table[aword] = 1\n",
      "                \n",
      "        return self.msg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now it is time to open the json file and analyze it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "json_data=open('./nnano-2012.json')\n",
      "data = json.load(json_data)\n",
      "word_count = WordCount()\n",
      "\n",
      "split_string = ''\n",
      "for entry in data:\n",
      "    if entry[u'dc:description']:\n",
      "        split_string = entry[u'dc:description'].replace('<p>', '').replace('</p>', '').replace(',','').replace('.','') \\\n",
      "                        .replace('\\'','').replace('(','').replace(')','').replace('-', ' ').lower()\n",
      "        word_count.append(split_string)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here I have the need to sort word_table by each key value. So I take this chance and start to learn a bit about NumPy, SciPy and Pandas."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "#from pandas import *\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index. The basic method to create a Series is to call:\n",
      "\n",
      "```\n",
      "s = Series(data, index=index)\n",
      "```\n",
      "\n",
      "Here, data can be many different things:\n",
      "\n",
      "- a Python dict\n",
      "- an ndarray\n",
      "- a scalar value (like 5)\n",
      "\n",
      "Here in this app, we use dict. Before counting the actual science words, I need to create a table containing all the \"normal\" english words such as 'as', 'of', 'but'. These words should be ignored in the counting. The word table will be screened against this normal word table to eliminate these words first. This is done inthe WordCount class's ignore_list - well, after some test, I found that the list can go so long that it is not practical to manually list it. Need some algo to construct such a list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = pd.Series(word_count.word_table)\n",
      "print s.order()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "+                            1\n",
        "mesoscopic                   1\n",
        "met                          1\n",
        "metabolism                   1\n",
        "metal:chalcogen              1\n",
        "metallicity                  1\n",
        "metallized                   1\n",
        "metal\u2013oxide\u2013semiconductor    1\n",
        "merit                        1\n",
        "metamaterial                 1\n",
        "methodologies                1\n",
        "metre                        1\n",
        "metrology                    1\n",
        "micelle                      1\n",
        "michelle                     1\n",
        "...\n",
        "by       164\n",
        "for      178\n",
        "are      190\n",
        "be       191\n",
        "can      206\n",
        "we       211\n",
        "is       217\n",
        "with     224\n",
        "that     297\n",
        "in       450\n",
        "to       514\n",
        "a        575\n",
        "and      751\n",
        "of       915\n",
        "the     1211\n",
        "Length: 4141, dtype: int64\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I can use the order() and sort() function of a Series to sort the Series by value. order() returns a new array and sort() is in place."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}